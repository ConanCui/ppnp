{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from ppnp.pytorch import PPNP\n",
    "from ppnp.pytorch.training import train_model\n",
    "from ppnp.pytorch.earlystopping import stopping_args\n",
    "from ppnp.pytorch.propagation import PPRExact, PPRPowerIteration\n",
    "from ppnp.data.io import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "        format='%(asctime)s: %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S',\n",
    "        level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "\n",
    "First we need to load the dataset we want to train on. The datasets used are in the `SparseGraph` format. This is just a class providing the adjacency, attribute and label matrices in a dense (`np.ndarray`) or sparse (`scipy.sparse.csr_matrix`) matrix format and some (in principle unnecessary) convenience functions. If you want to use external datasets, you can e.g. use the `networkx_to_sparsegraph` method in `ppnp.data.io` for converting NetworkX graphs to our SparseGraph format.\n",
    "\n",
    "The four datasets from the paper (Cora-ML, Citeseer, PubMed and MS Academic) can be found in the directory `data`.\n",
    "\n",
    "For this example we choose the Cora-ML graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Undirected, unweighted and connected SparseGraph with 15962 edges (no self-loops). Data: adj_matrix (2810x2810), attr_matrix (2810x2879), labels (2810), node_names (2810), attr_names (2879), class_names (7)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_name = 'cora_ml'\n",
    "graph = load_dataset(graph_name)\n",
    "graph.standardize(select_lcc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up propagation\n",
    "\n",
    "Next we need to set up the proper propagation scheme. In the paper we've introduced the exact PPR propagation used in PPNP and the PPR power iteration propagation used in APPNP.\n",
    "\n",
    "Here we use the hyperparameters from the paper. Note that we should use a different `alpha = 0.2` for MS Academic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_ppnp = PPRExact(graph.adj_matrix, alpha=0.1)\n",
    "prop_appnp = PPRPowerIteration(graph.adj_matrix, alpha=0.1, niter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose model hyperparameters\n",
    "\n",
    "Now we choose the hyperparameters. These are the ones used in the paper for all datasets.\n",
    "\n",
    "Note that we choose the propagation for APPNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'hiddenunits': [64],\n",
    "    'drop_prob': 0.5,\n",
    "    'propagation': prop_appnp}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "\n",
    "Now we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_split_args = {'ntrain_per_class': 20, 'nstopping': 500, 'nknown': 1500, 'seed': 2413340114}\n",
    "reg_lambda = 5e-3\n",
    "learning_rate = 0.01\n",
    "\n",
    "test = False\n",
    "device = 'cuda'\n",
    "print_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-02-26 15:53:49: PPNP: {'hiddenunits': [64], 'drop_prob': 0.5, 'propagation': PPRPowerIteration()}\n",
      "2019-02-26 15:53:49: PyTorch seed: 3108412824\n",
      "2019-02-26 15:53:53: Epoch 0: Train loss = 2.00, train acc = 15.7, early stopping loss = 1.96, early stopping acc = 58.0 (0.093 sec)\n",
      "2019-02-26 15:53:54: Epoch 20: Train loss = 1.62, train acc = 93.6, early stopping loss = 1.76, early stopping acc = 76.8 (1.449 sec)\n",
      "2019-02-26 15:53:56: Epoch 40: Train loss = 1.18, train acc = 99.3, early stopping loss = 1.45, early stopping acc = 80.4 (1.336 sec)\n",
      "2019-02-26 15:53:57: Epoch 60: Train loss = 0.92, train acc = 98.6, early stopping loss = 1.26, early stopping acc = 81.4 (1.413 sec)\n",
      "2019-02-26 15:53:58: Epoch 80: Train loss = 0.78, train acc = 99.3, early stopping loss = 1.14, early stopping acc = 82.4 (1.463 sec)\n",
      "2019-02-26 15:54:00: Epoch 100: Train loss = 0.70, train acc = 100.0, early stopping loss = 1.08, early stopping acc = 82.2 (1.478 sec)\n",
      "2019-02-26 15:54:01: Epoch 120: Train loss = 0.62, train acc = 100.0, early stopping loss = 1.03, early stopping acc = 82.8 (1.444 sec)\n",
      "2019-02-26 15:54:03: Epoch 140: Train loss = 0.57, train acc = 100.0, early stopping loss = 0.99, early stopping acc = 82.6 (1.450 sec)\n",
      "2019-02-26 15:54:04: Epoch 160: Train loss = 0.56, train acc = 99.3, early stopping loss = 0.94, early stopping acc = 81.8 (1.451 sec)\n",
      "2019-02-26 15:54:06: Epoch 180: Train loss = 0.52, train acc = 98.6, early stopping loss = 0.92, early stopping acc = 83.2 (1.457 sec)\n",
      "2019-02-26 15:54:07: Epoch 200: Train loss = 0.50, train acc = 100.0, early stopping loss = 0.89, early stopping acc = 82.0 (1.490 sec)\n",
      "2019-02-26 15:54:09: Epoch 220: Train loss = 0.49, train acc = 99.3, early stopping loss = 0.88, early stopping acc = 81.2 (1.454 sec)\n",
      "2019-02-26 15:54:10: Epoch 240: Train loss = 0.46, train acc = 100.0, early stopping loss = 0.91, early stopping acc = 80.4 (1.450 sec)\n",
      "2019-02-26 15:54:12: Epoch 260: Train loss = 0.44, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 81.6 (1.443 sec)\n",
      "2019-02-26 15:54:13: Epoch 280: Train loss = 0.42, train acc = 100.0, early stopping loss = 0.85, early stopping acc = 81.6 (1.470 sec)\n",
      "2019-02-26 15:54:14: Epoch 300: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.84, early stopping acc = 81.4 (1.440 sec)\n",
      "2019-02-26 15:54:16: Epoch 320: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.83, early stopping acc = 81.2 (1.453 sec)\n",
      "2019-02-26 15:54:17: Epoch 340: Train loss = 0.39, train acc = 97.9, early stopping loss = 0.83, early stopping acc = 82.4 (1.451 sec)\n",
      "2019-02-26 15:54:19: Epoch 360: Train loss = 0.38, train acc = 100.0, early stopping loss = 0.82, early stopping acc = 81.2 (1.443 sec)\n",
      "2019-02-26 15:54:20: Epoch 380: Train loss = 0.35, train acc = 100.0, early stopping loss = 0.80, early stopping acc = 82.4 (1.485 sec)\n",
      "2019-02-26 15:54:22: Epoch 400: Train loss = 0.34, train acc = 100.0, early stopping loss = 0.77, early stopping acc = 83.0 (1.440 sec)\n",
      "2019-02-26 15:54:23: Epoch 420: Train loss = 0.33, train acc = 100.0, early stopping loss = 0.77, early stopping acc = 81.6 (1.443 sec)\n",
      "2019-02-26 15:54:25: Epoch 440: Train loss = 0.32, train acc = 100.0, early stopping loss = 0.77, early stopping acc = 82.0 (1.442 sec)\n",
      "2019-02-26 15:54:26: Epoch 460: Train loss = 0.33, train acc = 100.0, early stopping loss = 0.76, early stopping acc = 82.8 (1.465 sec)\n",
      "2019-02-26 15:54:28: Epoch 480: Train loss = 0.34, train acc = 100.0, early stopping loss = 0.79, early stopping acc = 81.2 (1.442 sec)\n",
      "2019-02-26 15:54:29: Epoch 500: Train loss = 0.30, train acc = 100.0, early stopping loss = 0.79, early stopping acc = 82.6 (1.435 sec)\n",
      "2019-02-26 15:54:30: Epoch 520: Train loss = 0.31, train acc = 100.0, early stopping loss = 0.74, early stopping acc = 82.8 (1.455 sec)\n",
      "2019-02-26 15:54:32: Epoch 540: Train loss = 0.29, train acc = 100.0, early stopping loss = 0.73, early stopping acc = 83.0 (1.443 sec)\n",
      "2019-02-26 15:54:33: Epoch 560: Train loss = 0.28, train acc = 99.3, early stopping loss = 0.74, early stopping acc = 83.0 (1.471 sec)\n",
      "2019-02-26 15:54:35: Epoch 580: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.76, early stopping acc = 81.4 (1.441 sec)\n",
      "2019-02-26 15:54:36: Epoch 600: Train loss = 0.28, train acc = 100.0, early stopping loss = 0.71, early stopping acc = 83.6 (1.439 sec)\n",
      "2019-02-26 15:54:38: Epoch 620: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.76, early stopping acc = 80.4 (1.443 sec)\n",
      "2019-02-26 15:54:39: Epoch 640: Train loss = 0.28, train acc = 99.3, early stopping loss = 0.73, early stopping acc = 81.8 (1.468 sec)\n",
      "2019-02-26 15:54:41: Epoch 660: Train loss = 0.26, train acc = 100.0, early stopping loss = 0.70, early stopping acc = 83.0 (1.381 sec)\n",
      "2019-02-26 15:54:42: Epoch 680: Train loss = 0.26, train acc = 100.0, early stopping loss = 0.73, early stopping acc = 82.4 (1.373 sec)\n",
      "2019-02-26 15:54:43: Epoch 700: Train loss = 0.27, train acc = 100.0, early stopping loss = 0.73, early stopping acc = 83.2 (1.376 sec)\n",
      "2019-02-26 15:54:45: Epoch 720: Train loss = 0.26, train acc = 99.3, early stopping loss = 0.68, early stopping acc = 82.8 (1.379 sec)\n",
      "2019-02-26 15:54:46: Epoch 740: Train loss = 0.28, train acc = 99.3, early stopping loss = 0.74, early stopping acc = 81.2 (1.412 sec)\n",
      "2019-02-26 15:54:47: Epoch 760: Train loss = 0.25, train acc = 100.0, early stopping loss = 0.72, early stopping acc = 82.2 (1.372 sec)\n",
      "2019-02-26 15:54:48: Last epoch: 767, best epoch: 568 (55.217 sec)\n",
      "2019-02-26 15:54:48: Early stopping accuracy: 84.8%\n",
      "2019-02-26 15:54:48: Validation accuracy: 84.0%\n"
     ]
    }
   ],
   "source": [
    "result = train_model(\n",
    "        graph_name, PPNP, graph, model_args, learning_rate, reg_lambda,\n",
    "        idx_split_args, stopping_args, test, device, None, print_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
